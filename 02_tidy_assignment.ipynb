{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef9f390b",
   "metadata": {},
   "source": [
    "(02-tidy)=\n",
    "# Day 2 (Live) Intro to Data Wrangling and Tidy Data\n",
    "*Author*: Zach del Rosario\n",
    "\n",
    "\n",
    "### Learning outcomes\n",
    "By working through this notebook, you will be able to:\n",
    "\n",
    "- Understand basic principles of working with data in software\n",
    "- State the basic ideas of tidy data\n",
    "- State the basic ideas of data wrangling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf328c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install calls for Google Colab\n",
    "!pip install matplotlib\n",
    "!pip install numpy>=1.15\n",
    "!pip install pandas>=1.3.0\n",
    "!pip install seaborn>=0.9\n",
    "!pip install scipy\n",
    "!pip install toolz\n",
    "!pip install networkx\n",
    "!pip install pathos>=0.2.7\n",
    "!pip install lolopy\n",
    "!pip install pymatgen==2021.2.8.1\n",
    "!pip install matminer\n",
    "!pip install sklearn\n",
    "!pip install statsmodels\n",
    "!pip install pyDOE\n",
    "!pip install umap-learn\n",
    "!pip install py-grama>=0.2.2\n",
    "!pip install plotnine\n",
    "!pip install plotly\n",
    "!pip install ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcdbc0c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this day exercise, we'll make use of the Pandas and Grama packages to work with data. [Pandas](https://pandas.pydata.org/) is a package for data analysis, and it supplies the DataFrame object for representing data. [Grama](https://github.com/zdelrosario/py_grama) builds on top of Pandas to provide pipeline-based tools for data and machine learning.\n",
    "\n",
    "This notebook focuses on introducing *ideas*, the evening notebook will teach you the *mechanics* of how to use these tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import grama as gr\n",
    "\n",
    "DF = gr.Intention()\n",
    "\n",
    "# For downloading data\n",
    "import os\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c9708",
   "metadata": {},
   "source": [
    "The following code downloads the same data you extracted in the previous day's Tabula exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename for local data\n",
    "filename_data = \"./tabula-weibull.csv\"\n",
    "\n",
    "# The following code downloads the data, or (after downloaded)\n",
    "# loads the data from a cached CSV on your machine\n",
    "if not os.path.exists(filename_data):\n",
    "    # Make request for data\n",
    "    url_data = \"https://raw.githubusercontent.com/zdelrosario/mi101/main/mi101/data/tabula-weibull1939-table4.csv\"\n",
    "    r = requests.get(url_data, allow_redirects=True)\n",
    "    open(filename_data, 'wb').write(r.content)\n",
    "    print(\"   Tabula-extracted data downloaded from public Google sheet\")\n",
    "else:\n",
    "    # Note data already exists\n",
    "    print(\"   Tabula-extracted data loaded locally\")\n",
    "    \n",
    "# Read the data into memory\n",
    "df_tabula = pd.read_csv(filename_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1bf022",
   "metadata": {},
   "source": [
    "These are data on the tensile strength of specimens of stearic acid and plaster-of-paris.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab716cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tabula\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b42980",
   "metadata": {},
   "source": [
    "Note that Pandas renamed some columns to avoid giving us duplicate column names. The names are useful for holding metadata, but shorter column names are far easier to work with in a computational environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52faee",
   "metadata": {},
   "source": [
    "### __Q1__: Complete the code below to rename the columns\n",
    "\n",
    "*Hint*: You can click-and-drag on the DataFrame printout above for a less error-prone way of giving the original column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef4f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Copy the original column names into the double-quote below\n",
    "#       to complete the code and rename the columns with shorter\n",
    "#       names.\n",
    "###\n",
    "\n",
    "df_q1 = (\n",
    "    df_tabula\n",
    "    >> gr.tf_rename(\n",
    "## TASK: Uncomment these lines and fill-in the new (short) column names\n",
    "#         obs_1=\"\",   # Observation Number, Block 1\n",
    "#         area_1=\"\",  # Specimen area, Block 1\n",
    "#         sigma_1=\"\", # Stress, Block 1\n",
    "#         obs_2=\"\",   # Observation Number, Block 2\n",
    "#         area_2=\"\",  # Specimen area, Block 2\n",
    "#         sigma_2=\"\", # Stress, Block 2\n",
    "\n",
    "    )\n",
    ")\n",
    "\n",
    "## NOTE: No need to edit, this will show your renamed data\n",
    "df_q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d6e6c",
   "metadata": {},
   "source": [
    "Use the following to check your work:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd1cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NO NEED TO EDIT; use this to check your work\n",
    "assert(set(df_q1.columns) == {\"obs_1\", \"area_1\", \"sigma_1\", \"obs_2\", \"area_2\", \"sigma_2\"})\n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208eb01",
   "metadata": {},
   "source": [
    "Now the column names are much shorter, but they are far less interpretable. To help keep track of what each column means, we can construct a *data dictionary*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef080b6",
   "metadata": {},
   "source": [
    "### __Q2__: Complete the *data dictionary* below to document the units associated with the short column names.\n",
    "\n",
    "*Note*: Weibull in his (1939) paper reports these stress values in units `kg / mm^2`. This is strange; we'll return to this later!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2eed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tabula.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe5020",
   "metadata": {},
   "source": [
    "\n",
    "| Column | Units |\n",
    "|--------|-------|\n",
    "| `obs_1`   | (Unitless) |\n",
    "| `area_1`, | ??? |\n",
    "| `sigma_1` | ??? |\n",
    "| `obs_2`   | ??? |\n",
    "| `area_2`, | ??? |\n",
    "| `sigma_2` | ??? |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd587b5c",
   "metadata": {},
   "source": [
    "## Pivoting\n",
    "\n",
    "Note that the data are in a rather strange layout; the first three columns report the specimen number (`obs_1`), cross-sectional area (`area_1`), and measured stress (`sigma_1`) for all the observations up to `13`. Then the last three columns report the same quantities starting at observation `14`. This sort of layout with \"two blocks\" is *convenient* for reporting values in a compact form, but it is *highly inconvenient* for doing analysis, as we'll see next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b425f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f5bd8",
   "metadata": {},
   "source": [
    "Imagine we wanted to compute some simple statistics on these data, say the mean of the stress values. Since the data come in two blocks, we have to access them separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_mean_1 = df_q1.sigma_1.mean()\n",
    "sigma_mean_2 = df_q1.sigma_2.mean()\n",
    "\n",
    "print(\"Mean 1: {0:4.3f}\".format(sigma_mean_1))\n",
    "print(\"Mean 2: {0:4.3f}\".format(sigma_mean_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b08668e",
   "metadata": {},
   "source": [
    "We could do something hacky to combine the two:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a23827",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_mean_both = (12 * sigma_mean_1 + 11 * sigma_mean_2) / (12 + 11)\n",
    "print(\"Mean both: {0:4.3f}\".format(sigma_mean_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5712500",
   "metadata": {},
   "source": [
    "But it would be far easier if we could just *combine* all the relevant columns so they're not in \"blocks.\" This is what **pivoting** a dataset allows us to do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f07d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit; you'll learn this in the evening's notebook\n",
    "df_long = (\n",
    "    df_q1\n",
    "    >> gr.tf_pivot_longer(\n",
    "        columns=[\"obs_1\", \"area_1\", \"sigma_1\", \"obs_2\", \"area_2\", \"sigma_2\"],\n",
    "        names_to=[\".value\", \"block\"],\n",
    "        names_sep=\"_\",\n",
    "    )\n",
    "    >> gr.tf_arrange(DF.obs)\n",
    ")\n",
    "df_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214a182",
   "metadata": {},
   "source": [
    "Pivoting is one of the key tools we need to *tidy* our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e929347",
   "metadata": {},
   "source": [
    "## Tidy Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba26d5",
   "metadata": {},
   "source": [
    "*Tidy data* is a very simple---but very powerful---idea. The image below gives the definition of tidy data.\n",
    "\n",
    "![In tidy data: each variable forms a column, each observation forms a row, each cell is a single measurement.](https://raw.githubusercontent.com/zdelrosario/mi101/main/mi101/images/tidydata_1.jpg)\n",
    "\n",
    "Artwork by Allison Horst, [generously released](https://www.openscapes.org/blog/2020/10/12/tidy-data/) under an open-source license!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f8930",
   "metadata": {},
   "source": [
    "The big payoff of making your data tidy is that *you don't have to build special-purpose tools to work with your data*. Since every untidy dataset is untidy in its own unique (hideous) way, you would have to build special-purpose tools for every untidy dataset. By having one standard form of data, you can use the same tools for every dataset.\n",
    "\n",
    "![Comical cartoon comparing tools for tidy data and tools for untidy data.](https://raw.githubusercontent.com/zdelrosario/mi101/main/mi101/images/tidydata_3.jpg)\n",
    "\n",
    "Let's apply these ideas to the data we extracted using Tabula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7c586",
   "metadata": {},
   "source": [
    "### __Q3__: Inspect the original form of the data. Why are these data not tidy? How would you make the data tidy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4298aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NO NEED TO EDIT; run and inspect\n",
    "df_tabula.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40e4378",
   "metadata": {},
   "source": [
    "- (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad6d9a",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "Even once our data are *tidy*, they still might not be as usable as we might like. Another, less well-defined aspect of making data usable is *wrangling* the data into a more useful form. We'll talk about a few aspects we might need to wrangle our data to fix: unit conversion, filtering, and data types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63727943",
   "metadata": {},
   "source": [
    "### Unit conversion\n",
    "\n",
    "Weibull reports the $\\sigma_d$ values in `kg / mm^2`; if we interpret `kg` as a kilogram (mass) then these can't be stress values! However, suppose for a moment that he were using `kg` to denote a kilogram-force, where $1 \\text{kgf} = 1 \\text{kg} \\times 9.8 m/s^2$.\n",
    "\n",
    "Weibull gives a summary value for the same stress in the more interpretable units `g / (cm s^2)`. Let's check this hypothesis by comparing the proposed unit converstion with our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca45fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (540 x 10^5 g/(cm s^2)) / (980 cm/s^2) * (kg / 1000 g) * (cm^2 / 100 mm^2)\n",
    "print(\"Calculated: {0:4.3f}\".format(540e5 / 980 / 1000 / 100))\n",
    "df_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d5185",
   "metadata": {},
   "source": [
    "This is very near the `sigma` values we have in our dataset, which lends a great deal of credibility to our interpretation of `kg` as `kgf`. With this determined, we can make a unit conversation to more standard units.\n",
    "\n",
    "$$\\text{kgf} / \\text{mm}^2 = 9.8 \\text{MPa}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ec6dc",
   "metadata": {},
   "source": [
    "### __Q4__: Convert the units to MPa.\n",
    "\n",
    "Replace the factor of `1.0` below to convert the units to MPa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161e0c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Replace the 1.0 factor with the correct conversion factor\n",
    "###\n",
    "\n",
    "\n",
    "df_q4 = (\n",
    "    df_long\n",
    "    >> gr.tf_mutate(sigma_MPa=DF.sigma * 1.0) # Replace 1.0 with correct factor\n",
    "\n",
    ")\n",
    "\n",
    "df_q4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04308e32",
   "metadata": {},
   "source": [
    "### Filtering invalid values\n",
    "\n",
    "If we take a look at the end of the dataset, we see some strange `NaN` entries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67865e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "df_q4.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35839b0f",
   "metadata": {},
   "source": [
    "The value `NaN` is a special value that denotes *Not a Number*. Essentially, this is the way we represent something invalid. `NaN` can represent a missing value, a failed type conversion, or any of a variety of ways something can fail to be a number.\n",
    "\n",
    "Often, the way we deal with `NaN` values is simply to remove the offending row from our dataset. We can do that with a *filter* operation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "df_filtered = (\n",
    "    df_q4\n",
    "    >> gr.tf_filter(gr.not_nan(DF.sigma_MPa))\n",
    ")\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d26707",
   "metadata": {},
   "source": [
    "### Converting types\n",
    "\n",
    "Lastly, it's worth noting that all of our columns have a *data type* associated with them. We can inspect that type with the `dtypes` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "df_q4.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185464f",
   "metadata": {},
   "source": [
    "The `float64` is a way of representing a decimal value. The `area` and stress values we'd expect to be decimal values, but what's up with `block` and `obs`? We'd expect those to be integers. If the types of your columns are not what you'd expect, you can do a [type conversion](https://en.wikipedia.org/wiki/Type_conversion) to attempt a converstion from one data type to a desired type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75263239",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "df_converted = (\n",
    "    df_filtered\n",
    "    >> gr.tf_mutate(\n",
    "        block=gr.as_int(DF.block),\n",
    "        obs=gr.as_int(DF.obs),\n",
    "    )\n",
    ")\n",
    "df_converted.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dfc455",
   "metadata": {},
   "source": [
    "We'll see more about casting in the evening notebook. But now, we have a tidy and wrangled dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff32a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "df_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25deced",
   "metadata": {},
   "source": [
    "## Bonus: The Payoff of Tidying and Wrangling\n",
    "\n",
    "As a preview for why we want to learn how to tidy and wrangle data, let's see what some of these tidy data tools will allow us to do. The [plotnine](https://plotnine.readthedocs.io/en/stable/) package is a plotting package designed for tidy data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e826e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "import plotnine as pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208dda5d",
   "metadata": {},
   "source": [
    "Using plotnine, we can very quickly construct simple graphs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba80d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_converted\n",
    "    >> pt.ggplot(pt.aes(\"area\", \"sigma_MPa\"))\n",
    "    + pt.geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900780e0",
   "metadata": {},
   "source": [
    "Part of the power of plotnine (over other graphing software) is that we can very easily tweak our graphs to show additional information. For instance, if we also wanted to see if the samples in the two blocks were at all different, we could easily color the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f378db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_converted\n",
    "    >> pt.ggplot(pt.aes(\"area\", \"sigma_MPa\"))\n",
    "#     + pt.geom_point() # Original\n",
    "    + pt.geom_point(pt.aes(color=\"block\")) # Modified for color\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443e82c",
   "metadata": {},
   "source": [
    "We'll learn more about plotnine and visualization in tomorrow's workshop exercises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd582f26",
   "metadata": {},
   "source": [
    "# Survey\n",
    "\n",
    "---\n",
    "\n",
    "Once you complete this activity, please fill out the following 30-second survey:\n",
    "\n",
    "> [Survey link](https://forms.gle/yWZkqDXWSyVDWm41A)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
