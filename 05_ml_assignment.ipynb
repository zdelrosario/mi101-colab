{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0620556",
   "metadata": {},
   "source": [
    "(05-ml)=\n",
    "# Day 4 (Live) Exercises in Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "*Author*: Zach del Rosario\n",
    "\n",
    "The _primary_ purpose of this notebook is to help you *not get fooled by machine learning*! As Drew Conway notes, possessing hacking skills and substantive experience---but having no math or statistics background---puts one in the [danger zone](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram). While we can't possibly cover *everything* you need in a single workshop, this exercise will highlight some of the challenges of doing machine learning well. See the *Further Reading* section at the end for suggestions on learning more.\n",
    "\n",
    "### Learning outcomes\n",
    "\n",
    "By working through this notebook, you will be able to:\n",
    "\n",
    "- Use grama to work with models.\n",
    "- Understand the importance of *underfitting* and *overfitting*.\n",
    "- Use cross-validation to help avoid *underfitting* and *overfitting*.\n",
    "- State some example ways underfitting and overfitting show up studying a materials dataset.\n",
    "\n",
    "Tips:\n",
    "\n",
    "- This exercise heavily uses the [py-grama](https://github.com/zdelrosario/py_grama) package; you can find more info on the [documentation site](https://py-grama.readthedocs.io/en/latest/).\n",
    "- This exercise indirectly uses [scikit-learn](https://scikit-learn.org/stable/); you can find lots of useful info on this package on their [documentation site](https://scikit-learn.org/stable/documentation.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8443359",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install calls for Google Colab\n",
    "!pip install matplotlib\n",
    "!pip install numpy>=1.15\n",
    "!pip install seaborn>=0.9\n",
    "!pip install scipy\n",
    "!pip install toolz\n",
    "!pip install networkx\n",
    "!pip install pathos>=0.2.7\n",
    "!pip install lolopy\n",
    "!pip install pymatgen==2021.2.8.1\n",
    "!pip install matminer\n",
    "!pip install sklearn\n",
    "!pip install statsmodels\n",
    "!pip install pyDOE\n",
    "!pip install umap-learn\n",
    "!pip install py-grama>=0.2.2\n",
    "!pip install plotnine\n",
    "!pip install plotly\n",
    "!pip install ipympl\n",
    "!pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fa8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as pt\n",
    "import grama as gr\n",
    "\n",
    "DF = gr.Intention()\n",
    "\n",
    "# Show all pandas columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# For downloading data\n",
    "import os\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b90e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def add_noise(function, sigma=0.1, seed=101):\n",
    "    \"\"\"Add noise to deterministic functions\n",
    "    \"\"\"\n",
    "    def new_function(x):\n",
    "        np.random.seed(seed)\n",
    "        y = function(x)\n",
    "        return y + sigma * np.random.normal(size=y.shape)\n",
    "\n",
    "    return new_function\n",
    "\n",
    "# Reference points for regression examples\n",
    "X_ref = np.linspace(-1, +1, num=100)\n",
    "\n",
    "# Reference models\n",
    "def fcn_1(x): return (0.3 * x**2 + 1.0 * x + 2)\n",
    "def fcn_2(x): return (-2.0 * x**3 + 0.4 * x**2 + 1.0 * x + 2)\n",
    "\n",
    "fcn_1_noisy = gr.make_symbolic(add_noise(fcn_1))\n",
    "fcn_2_noisy = gr.make_symbolic(add_noise(fcn_2, sigma=0.2))\n",
    "\n",
    "# Package as a dataframe\n",
    "df_data = gr.df_make(\n",
    "    x=X_ref,\n",
    "    y_1=fcn_1_noisy(X_ref),\n",
    "    y_2=fcn_2_noisy(X_ref),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7fbf1",
   "metadata": {},
   "source": [
    "## Regression Helper Function\n",
    "\n",
    "---\n",
    "\n",
    "This function will automate some steps so we can focus on high-level ideas, but I define it here in case you'd like to see the details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For partial evaluation\n",
    "from toolz import curry\n",
    "## Model training tools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "## Encapsulate featurizer and regressor in one object\n",
    "class FunctionLM(gr.Function):\n",
    "    def __init__(self, regressor, var, out, name, order, runtime):\n",
    "        self.regressor = regressor\n",
    "        self.var = var\n",
    "        self.out = list(map(lambda s: s + \"_mean\", out))\n",
    "        self.name = name\n",
    "        self.order = order\n",
    "        self.runtime = runtime\n",
    "\n",
    "    def eval(self, df):\n",
    "        ## Check invariant; model inputs must be subset of df columns\n",
    "        if not set(self.var).issubset(set(df.columns)):\n",
    "            raise ValueError(\n",
    "                \"Model function `{}` var not a subset of given columns\".format(\n",
    "                    self.name\n",
    "                )\n",
    "            )\n",
    "\n",
    "        ## Featurize\n",
    "        x = np.atleast_2d(df[self.var].values)\n",
    "        poly = PolynomialFeatures(self.order)\n",
    "        X_feat = poly.fit_transform(x)\n",
    "        \n",
    "        ## Predict\n",
    "        y = self.regressor.predict(X_feat).flatten()\n",
    "        return pd.DataFrame(data=y, columns=self.out)\n",
    "\n",
    "## Fitting routine\n",
    "@curry\n",
    "def fit_regression(df, var=None, out=None, order=1):\n",
    "    r\"\"\"Fit a linear regression of specified order\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Data for fitting\n",
    "        var (iterable of str): Names of input variable (feature); must be column in df\n",
    "        out (iterable of str): Name of output variable (response); must be column in df\n",
    "        order (int): Polynomial order for fit\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(out) > 1:\n",
    "        raise ValueError(\"This simple helper can only handle one output.\")\n",
    "    # Featurize\n",
    "    x = np.atleast_2d(df[var].values)\n",
    "    poly = PolynomialFeatures(order)\n",
    "    X_feat = poly.fit_transform(x)\n",
    "    \n",
    "    # Fit regression\n",
    "    y = np.atleast_2d(df[out].values)\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_feat, y)\n",
    "    \n",
    "    # Package\n",
    "    fun = FunctionLM(lm, var, out, \"Linear Model\", order, 0)\n",
    "    \n",
    "    return gr.Model(functions=[fun], domain=None, density=None)\n",
    "    \n",
    "## Create pipe-enabled regression utility\n",
    "ft_regression = gr.add_pipe(fit_regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b32ce",
   "metadata": {},
   "source": [
    "## Underfitting and Overfitting\n",
    "\n",
    "---\n",
    "\n",
    "First we'll cover some key ideas on simple cases. These are not 'real' data, but the simplicity of the examples will allow us to focus on concepts.\n",
    "\n",
    "Here I generate some data from a simple polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "# Ground-truth data; no noise\n",
    "df_ex1 = (\n",
    "    gr.df_make(x=np.linspace(-1, +1, num=10))\n",
    "    >> gr.tf_mutate(y=fcn_1(DF.x))\n",
    ")\n",
    "\n",
    "# Plot\n",
    "(\n",
    "    df_ex1\n",
    "    >> pt.ggplot(pt.aes(\"x\", \"y\"))\n",
    "    + pt.geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b2fead",
   "metadata": {},
   "source": [
    "We will fit a simple [linear regression](https://en.wikipedia.org/wiki/Linear_regression) to these data. To do this, we'll use the linear regression helper `ft_regression()` (defined above). To start, we'll assume that the data were generated from an underlying rule (a _model_) of the form\n",
    "\n",
    "$$y_{\\text{mean}} = m x + b,$$\n",
    "\n",
    "and attempt to _learn_ the slope $m$ and intercept $b$ by _minimizing_ the difference between the measured values `y` and the predicted values `y_mean`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89307a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "# Fit the line\n",
    "md_line = (\n",
    "    df_ex1\n",
    "    >> ft_regression(var=[\"x\"], out=[\"y\"], order=1)\n",
    ")\n",
    "\n",
    "# Predict\n",
    "df_line_pred = (\n",
    "    md_line\n",
    "    >> gr.ev_df(df=df_ex1)\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "(\n",
    "    df_line_pred\n",
    "    >> pt.ggplot(pt.aes(\"x\", \"y_mean\"))\n",
    "    + pt.geom_line(linetype=\"dashed\", color=\"grey\")\n",
    "    + pt.geom_point(color=\"grey\")\n",
    "    + pt.geom_point(pt.aes(y=\"y\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1c38e9",
   "metadata": {},
   "source": [
    "The `ft_regression()` tool returns a *grama model*; let's learn about this toolset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047107b1",
   "metadata": {},
   "source": [
    "### Working with models in Grama\n",
    "\n",
    "In addition to the data *transformation* tools we used in previous notebooks, grama provides a *model object* to simplify working with different forms of model. The [grama documentation](https://py-grama.readthedocs.io/en/latest/) has more information, but let's focus on the high-level details.\n",
    "\n",
    "Like with a DataFrame, we can get a view of the model simply by printing it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit, run to show info on `md_line`\n",
    "md_line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd405c25",
   "metadata": {},
   "source": [
    "The text above is a summary of what's in the model. Based on this summary, we can see that it takes a single input `x` and maps it to a single output `y_mean`. This can be particularly useful for checking the \"basic facts\" about the model: what inputs does it require? What outputs does it provide?\n",
    "\n",
    "Note that we fit a model to predict an output `y`, but our model provides `y_mean`. This renaming is deliberate; we're providing a *predicted value*, not a *measured value*. Since our model is based on a [statistical mean](https://en.wikipedia.org/wiki/Linear_regression#Interpretation), the output gets the `_mean` suffix.\n",
    "\n",
    "One of the most important uses of a fitted model is to predict unobserved output values. We can do this with a grama model by providing values for the desired inputs and *evaluating* the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6961b58f",
   "metadata": {},
   "source": [
    "### __Q1__: Evaluate the model at new input values\n",
    "\n",
    "Use the function `gr.ev_df()` to evaluate the model `md_line` fitted above. Make use of the new set of input values `df_dense`. Answer the questions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc801dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Evaluate md_line at the df_dense input values.\n",
    "###\n",
    "\n",
    "# -- FINISH THE CODE BELOW -----\n",
    "df_dense = gr.df_make(x=np.linspace(-1, +1, num=100))\n",
    "\n",
    "df_evaluated = (\n",
    "    md_line\n",
    "## TODO: Evaluate the model here\n",
    "\n",
    ")\n",
    "\n",
    "# -- NO NEED TO EDIT BELOW HERE -----\n",
    "(\n",
    "    df_evaluated\n",
    "    >> gr.tf_mutate(source=\"Fit\")\n",
    "    >> pt.ggplot(pt.aes(\"x\", \"y_mean\", color=\"source\"))\n",
    "    + pt.geom_line(linetype=\"dashed\")\n",
    "    + pt.geom_point()\n",
    "    + pt.geom_point(\n",
    "        data=df_ex1\n",
    "        >> gr.tf_mutate(source=\"Measured\"),\n",
    "        mapping=pt.aes(y=\"y\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152cb92",
   "metadata": {},
   "source": [
    "*Observe*\n",
    "\n",
    "- How many points are measured? How many are fit? (Note: Don't count by hand, try looking at the code!)\n",
    "  - (Your response here)\n",
    "- Suppose we wanted to evaluate the quality of our fit. At how many locations could we evaluate the quality of the fit? Why?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18da264",
   "metadata": {},
   "source": [
    "### Model Flexibility and _Underfitting_\n",
    "\n",
    "Note that we had to _assume_ a model-form in order to do the fitting: $y_{\\text{mean}} = m x + b$. From the figure above, we can see that the model is close to the true values, but obviously lacks the curvature of the true data-generating process.\n",
    "\n",
    "This phenomenon---the failure of a model to capture behavior in the data---is called _underfitting_. To reduce underfitting, we need to make our model _more flexible_. For a linear regression, we can do this by **increasing the order** of the regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e883fe",
   "metadata": {},
   "source": [
    "### __Q2__: Fit a quadratic model to the data\n",
    "\n",
    "Use the `ft_regression()` helper to fit a quadratic model (`order = 2`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af091252",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Fit a quadratic model\n",
    "###\n",
    "\n",
    "# -- FINISH THE CODE BELOW -----\n",
    "# Fit a quadratic\n",
    "md_quad = (\n",
    "    df_ex1\n",
    "## TODO: Fit a regression here\n",
    "\n",
    ")\n",
    "\n",
    "# -- NO NEED TO EDIT BELOW HERE -----\n",
    "# Predict\n",
    "df_quad_pred = (\n",
    "    md_quad\n",
    "    >> gr.ev_df(df=df_ex1)\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "(\n",
    "    df_quad_pred\n",
    "    >> pt.ggplot(pt.aes(\"x\", \"y_mean\"))\n",
    "    + pt.geom_line(linetype=\"dashed\")\n",
    "    + pt.geom_point(pt.aes(y=\"y\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5dec2",
   "metadata": {},
   "source": [
    "Here we can see the model fits the data _perfectly_. This _suggests_ that we have successfully discovered the _exact_ rule that generated these data, which in _this special case happens to be true_.\n",
    "\n",
    "However, we will very rarely be able to fit the true function exactly. This is because real data tend to have _noise_, which corrupts the underlying function we are trying to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228173c4",
   "metadata": {},
   "source": [
    "### Noise and _Overfitting_\n",
    "\n",
    "Below, I generate data from the same model, but add a little bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9474b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "# Ground-truth data; now with noise\n",
    "df_ex2 = (\n",
    "    gr.df_make(x=np.linspace(-1, +1, num=10))\n",
    "    >> gr.tf_mutate(y=fcn_1_noisy(DF.x))\n",
    ")\n",
    "\n",
    "# Plot\n",
    "(\n",
    "    df_ex2\n",
    "    >> pt.ggplot(pt.aes(\"x\", \"y\"))\n",
    "    + pt.geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd6998",
   "metadata": {},
   "source": [
    "Let's see what happens when we fit a quadratic to the noisy data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_ex2\n",
    "    >> ft_regression(var=[\"x\"], out=[\"y\"], order=2)\n",
    "    >> gr.ev_df(df=df_ex2)\n",
    "    \n",
    "    >> pt.ggplot(pt.aes(\"x\", \"y_mean\"))\n",
    "    + pt.geom_line(linetype=\"dashed\")\n",
    "    + pt.geom_point(pt.aes(y=\"y\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fcc56",
   "metadata": {},
   "source": [
    "Here we can see that the fit is no longer perfect, despite coming from the \"same\" model. Since we already know that a quadratic can fit the underlying function perfectly, underfitting is not the issue here. Instead, the error is increased due to the noise in the data.\n",
    "\n",
    "_However_, we have not yet seen a case of _overfitting_. To see that phenomenon, let's keep increasing the order of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe21a1f4",
   "metadata": {},
   "source": [
    "### __Q3__: Overfit the model\n",
    "\n",
    "Increase the `order` of the regression until the model goes through every measured point. Does this seem like a reasonable model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Increase the order until the fit is perfect\n",
    "###\n",
    "\n",
    "# -- FINISH THE CODE BELOW -----\n",
    "(\n",
    "    df_ex2\n",
    "    >> ft_regression(\n",
    "        var=[\"x\"], \n",
    "        out=[\"y\"], \n",
    "        order=2 # TODO: Increase the order until the fit is perfect\n",
    "    )\n",
    "    >> gr.ev_df(df=gr.df_make(x=np.linspace(-1, +1, num=100)))\n",
    "    \n",
    "    >> pt.ggplot(pt.aes(\"x\", \"y_mean\"))\n",
    "    + pt.geom_line(linetype=\"dashed\")\n",
    "    + pt.geom_point(\n",
    "        data=df_ex2,\n",
    "        mapping=pt.aes(y=\"y\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe45133",
   "metadata": {},
   "source": [
    "### Balancing Overfitting and Underfitting\n",
    "\n",
    "To help illustrate, let's look at one more synthetic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3805d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "# More complicated function with noise\n",
    "df_ex3 = (\n",
    "    gr.df_make(x=np.linspace(-1, +1, num=10))\n",
    "    >> gr.tf_mutate(y=fcn_2_noisy(DF.x))\n",
    ")\n",
    "\n",
    "(\n",
    "    df_ex3\n",
    "    >> pt.ggplot(pt.aes(\"x\", \"y\"))\n",
    "    + pt.geom_point()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2560c5",
   "metadata": {},
   "source": [
    "Here we can see a somewhat complicated function that is quite corrupted by noise. Below, I'm going to fit a number of polynomial models of different orders. In practice, we would like to _make a decision_ about what polynomial order to use. A sensible choice would be to pick the order that minimizes the error---let's see which model accomplishes this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DEMONSTRATION CODE, NO NEED TO EDIT -----\n",
    "# Fit on many different orders\n",
    "df_train_err = pd.DataFrame()\n",
    "df_test_err = pd.DataFrame()\n",
    "for order in range(1, 10):\n",
    "    # Fit the model\n",
    "    md_tmp = (\n",
    "        df_ex3\n",
    "        >> ft_regression(var=[\"x\"], out=[\"y\"], order=order)\n",
    "    )\n",
    "    # Evaluate model on training data\n",
    "    df_tmp = (\n",
    "        md_tmp\n",
    "        >> gr.ev_df(df=df_ex3)\n",
    "        >> gr.tf_mutate(order=order)\n",
    "    )\n",
    "    df_train_err = pd.concat((df_train_err, df_tmp), axis=0)\n",
    "    # Evaluate model on test data\n",
    "    df_tmp = (\n",
    "        gr.df_make(x=np.linspace(-1, +1, num=100))\n",
    "        >> gr.tf_mutate(y=fcn_2(DF.x), order=order)\n",
    "        >> gr.tf_md(md=md_tmp)\n",
    "    )\n",
    "    df_test_err = pd.concat((df_test_err, df_tmp), axis=0)\n",
    "\n",
    "# Visualize a few cases\n",
    "(\n",
    "    df_train_err\n",
    "    >> gr.tf_filter(gr.var_in(DF.order, [1, 3, 9]))\n",
    "    >> pt.ggplot(pt.aes(\"x\", \"y_mean\"))\n",
    "    + pt.geom_line(pt.aes(color=\"factor(order)\"), linetype=\"dashed\")\n",
    "    + pt.geom_point(data=df_ex3, mapping=pt.aes(y=\"y\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4effca",
   "metadata": {},
   "source": [
    "Here I've selected just a few of the models to plot. We can see\n",
    "\n",
    "- The `Order = 1` case is **underfit**, like we saw in the example above\n",
    "- The `Order = 9` case curves tortuously to go through many points; this is an example of **overfitting**\n",
    "- The `Order = 3` case is not perfect, but tends to balance between underfitting and overfitting. This is a well-fit model.\n",
    "\n",
    "More generally, _overfitting_ is when the model fits to spurrious patterns in the data; essentially, we are fitting to noise, rather than signal. We would like to detect and avoid overfitting in practice! While we can see above some suspicious behavior based on the fitted curves, we might like a _quantitative_ way to compare models; in particular, once we have multiple inputs we won't be able to create simple `y vs x` plots like we do above. \n",
    "\n",
    "Additionally, there's a *subtle* issue that we'll run into when it comes to evaluating the accuracy of a model: The `Order = 9` case *clearly* looks unstable and untrustworthy, but note that the model is *perfect* at the points where we have measured output values. This is going to present some issues that we'll see below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb668463",
   "metadata": {},
   "source": [
    "## Quantifying Error and Cross-Validating\n",
    "\n",
    "---\n",
    "\n",
    "There are a wide variety of ways to quantify the accuracy of a model: The code below computes three example metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d447ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_train_err\n",
    "    >> gr.tf_group_by(DF.order)\n",
    "    >> gr.tf_summarize(\n",
    "        mse=gr.rmse(DF.y_mean, DF.y),\n",
    "        ndme=gr.ndme(DF.y_mean, DF.y),\n",
    "        rsq=gr.rsq(DF.y_mean, DF.y),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261edc7",
   "metadata": {},
   "source": [
    "\n",
    "- `mse` is [mean-squared error](https://en.wikipedia.org/wiki/Mean_squared_error), a common error metric often used in the machine learning community. Smaller values are more accurate.\n",
    "- `ndme` stands for *non-dimensional model error*. As the name implies, this quantity is dimensionless, and takes a value of `0` when the model is perfect, and a value of `1` (sometimes higher) when the model is uninformative. This metric is not terribly common, but it is very *useful* as a dimensionless metric for error.\n",
    "- `rsq` is short for $R^2$ (r-squared), also called the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination), a common *goodness of fit* metric used in the statistics community. Higher values are more accurate, with $R^2 = 1$ indicating a perfect fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28053e",
   "metadata": {},
   "source": [
    "### Train vs Test\n",
    "\n",
    "Remember above how the `order = 9` model looked untrustworthy, but went through all of our observed data points? Let's quantify the error both on the available data (the `train` data), but check the model's behavior on an independent set of observations (the `test` data). The code below plots error estimated using these two different sets of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_train_err\n",
    "    >> gr.tf_mutate(source=\"train\")\n",
    "    >> gr.tf_bind_rows(\n",
    "        df_test_err\n",
    "        >> gr.tf_mutate(source=\"test\")\n",
    "    )\n",
    "    >> gr.tf_group_by(DF.order, DF.source)\n",
    "    >> gr.tf_summarize(ndme=gr.ndme(DF.y_mean, DF.y))\n",
    "    \n",
    "    >> pt.ggplot(pt.aes(\"order\", \"ndme\"))\n",
    "    + pt.geom_line(pt.aes(linetype=\"source\"))\n",
    "    + pt.scale_y_log10()\n",
    "    + pt.labs(\n",
    "        x=\"Polynomial Order\",\n",
    "        y=\"NDME\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f64487",
   "metadata": {},
   "source": [
    "Here we can see the `train` and `test` error values _greatly diverge_. This is _highly problematic_ for two interrelated reasons:\n",
    "\n",
    "1. In practice, we would only have access to the `train` curve, as the `test` curve relies on extra data (that we don't have).\n",
    "2. If we were to make a decision about `Polynomial Order` based on the `train` curve, we would choose a much higher order than what would minimize the NDME in the `True` case.\n",
    "\n",
    "The underlying reason for the poor error estimate here is that _we are using the same data to both train and test the model_. We can improve our estimates for the error through various techniques; below, we will use the technique of _cross-validation_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f631ece",
   "metadata": {},
   "source": [
    "### Avoiding Optimistic Estimates: Cross-Validation\n",
    "\n",
    "[Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is one technique for estimating the error in a way that avoids the \"optimism\" we saw above. For the variant _k-fold cross-validation_ (k-fold CV), we split all our data into _folds_, and use these to build _training_ and _test_ sets. Generally:\n",
    "\n",
    "* _Training_ data are used to fit a model\n",
    "* _Test_ data are used to evaluate a model\n",
    "\n",
    "![CV schematic](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "\n",
    "(Schematic for k-fold CV. Fabian Flock, via Wikimedia)\n",
    "\n",
    "In each of our $k$ iterations, we _do not allow_ the model to see a test fold (`Test data` above) during training, and fit the model only on the remaining data (`Training data` above). After training, we compute our chosen error metric on the test fold. Finally, we repeat this process on each of the $k$ chosen folds. This gives us a set of less optimistic estimates for the error, which we can summarize e.g. as a mean CV error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed325f7",
   "metadata": {},
   "source": [
    "This procedure is implemented in the grama function `tf_kfolds()`, as demonstrated below. Note that `tf_kfolds()` takes in a dataset to use for k-fold cross validation, as well as a *fitting procedure* that we will use for each iteration. The routine also provides some reasonable default values for the number of folds `k` and the error metrics to compute. We can override these by providing keyword arguments (as you'll do below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ca4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "(\n",
    "    df_ex3\n",
    "    >> gr.tf_kfolds(\n",
    "        ft=ft_regression(var=[\"x\"], out=[\"y\"], order=9),\n",
    "#         summaries=dict(mse=gr.mse), # Uncomment to override defaults\n",
    "#         k=3,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f33413",
   "metadata": {},
   "source": [
    "Note that the k-fold CV estimated `rsq_y` values are *abysmal*; they take negative values, which indicates that the model performance is quite bad.\n",
    "\n",
    "One powerful use of k-fold CV is to do a *hyperparameter study*: In our case, we'll study how the model accuracy changes as we sweep over the polynomial order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09978f70",
   "metadata": {},
   "source": [
    "### __Q4__: Override the defaults\n",
    "\n",
    "Complete the code below by overriding the default settings for `tf_kfolds()`. Provide an estimate of the non-dimensional model error (`gr.ndme`). You can also try changing the value of `k` to see how the results change.\n",
    "\n",
    "Answer the questions under *Observations* below.\n",
    "\n",
    "*Hint:* You may find that the code throws errors for larger values of `k`. Think about how many data points you have available in `df_ex3`. How many folds can you split the data into while having more than just one point in each fold?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3826bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Increase the order until the fit is perfect\n",
    "###\n",
    "\n",
    "# -- FINISH THE CODE BELOW -----\n",
    "# Fit on many different orders\n",
    "df_kfolds = pd.DataFrame()\n",
    "# Iterate over a selection of polynomial orders\n",
    "for order in range(1, 10):\n",
    "    # Evaluate model with k-fold CV\n",
    "    df_tmp = (\n",
    "        df_ex3\n",
    "        >> gr.tf_kfolds(\n",
    "            # Use the desired polynomial order for this iteration\n",
    "            ft=ft_regression(var=[\"x\"], out=[\"y\"], order=order),\n",
    "            ## TODO: Override the defaults in tf_kfolds\n",
    "\n",
    "        )\n",
    "        # Record the order used in fitting\n",
    "        >> gr.tf_mutate(order=order)\n",
    "    )\n",
    "    # Record the results for this polynomial order\n",
    "    df_kfolds = pd.concat((df_kfolds, df_tmp), axis=0)\n",
    "    \n",
    "# Visualize\n",
    "(\n",
    "    df_kfolds\n",
    "    >> pt.ggplot(pt.aes(\"order\", \"ndme_y\"))\n",
    "    + pt.geom_boxplot(pt.aes(group=\"order\"))\n",
    "    + pt.scale_x_continuous(breaks=range(1, 10))\n",
    "    + pt.scale_y_log10()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b48a4",
   "metadata": {},
   "source": [
    "*Observe*\n",
    "\n",
    "- How much do the results change as you change `k`?\n",
    "  - (Your response here)\n",
    "\n",
    "- What polynomial `order` would you select, *based on the k-fold CV results*? Why?\n",
    "  - (Your response here)\n",
    "\n",
    "- Inspect the function definition of `fcn_2` at the top of this notebook. What is the *true* polynomial order of the underlying function?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14138b9d",
   "metadata": {},
   "source": [
    "Here we have just one tunable knob (polynomial order) that defines our model. More generally, these kinds of user-selected quantities are called [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)). Cross-validation and related techniques are key to _tuning hyperparameters_, and generally making choices about what sort of model we ought to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b0355",
   "metadata": {},
   "source": [
    "```{admonition} Terminology Alert\n",
    "Across various communities, the terms \"train\", \"test\", and \"validation\" data are used somewhat inconsistently. The term \"train\" is used most consistently: The training data is a subset of data used to fit the model. However different authors use the terms \"test\" and \"validation\" interchangeably. \n",
    "\n",
    "What's truly important is that you keep in mind the **purpose** of data subsets: Which subset is being used to fit the model? (Train) Which subset is being used to estimate error for hyperparameter tuning? (Test or Validation) Do you have a third disjoint subset to give an unbiased error estimate? (Validation or Test). For more information, see the relevant [Wikipedia article](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a7a543",
   "metadata": {},
   "source": [
    "## Case Study: Steel Fatigue Strength Prediction\n",
    "\n",
    "---\n",
    "\n",
    "So far, we've looked at fairly artificial modeling problems. To help illustrate these fitting and cross-validation ideas in a real materials problem, let's look at predicting the fatigue strength of steel alloys.\n",
    "\n",
    "The data for this case study come from [Agrawal et al. (2014) *IMMI*](https://link.springer.com/article/10.1186/2193-9772-3-8).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f46297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename for local data\n",
    "filename_data = \"./agrawal_data.csv\"\n",
    "\n",
    "# The following code downloads the data, or (after downloaded)\n",
    "# loads the data from a cached CSV on your machine\n",
    "if not os.path.exists(filename_data):\n",
    "    # Make request for data\n",
    "    url_data = \"https://raw.githubusercontent.com/zdelrosario/mi101/main/mi101/data/agrawal_data.csv\"\n",
    "    r = requests.get(url_data, allow_redirects=True)\n",
    "    open(filename_data, 'wb').write(r.content)\n",
    "    print(\"   Alloy data downloaded from public GitHub file\")\n",
    "else:\n",
    "    # Note data already exists\n",
    "    print(\"    Alloy data loaded locally\")\n",
    "    \n",
    "# Read the data into memory\n",
    "df_raw = pd.read_csv(filename_data)\n",
    "\n",
    "# Drop the index columns\n",
    "df_fatigue_data = (\n",
    "    df_raw\n",
    "    >> gr.tf_drop(\"Unnamed: 0\", \"Sample Number\")\n",
    "    >> gr.tf_select(\"Fatigue Strength\", \"chemical_formula\", gr.everything())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9af546",
   "metadata": {},
   "source": [
    "As always, we start by doing basic checks on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1daf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit\n",
    "df_fatigue_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984005dc",
   "metadata": {},
   "source": [
    "We have the outcome column that we seek to model (`Fatigue Strength`, at $10^7$ cycles), the chemical composition of the alloy in string-form, and a *large* number of processing characteristics.\n",
    "\n",
    "We can inspect a histogram of the fatigue strength to get a sense for the variation in the quantity we seek to model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e67635",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_fatigue_data\n",
    "    >> pt.ggplot(pt.aes(\"Fatigue Strength\"))\n",
    "    + pt.geom_histogram(bins=60)\n",
    "    + pt.theme_minimal()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f151161",
   "metadata": {},
   "source": [
    "The fatigue strength is *highly variable*, with a large bulk around `500`, but values as low as `250` and over `1000` as well.\n",
    "\n",
    "*NB* I had difficulty finding the units for the reported Fatigue Strength in the [original publication](https://link.springer.com/article/10.1186/2193-9772-3-8#Abs1). Presumably these values are given in `MPa`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2e812",
   "metadata": {},
   "source": [
    "This dataset has *many* columns; for modeling our first job should be to sort out what we want to predict, and which columns (*features*) we should use to predict that quantity. Our linear regression implementation can only handle *numeric* columns, so let's inspect the datatypes to see which features our model can handle:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fatigue_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d560a",
   "metadata": {},
   "source": [
    "The majority of the columns are numeric, *except* for the `chemical_formula`. To start, let's try modeling the strength without any composition information. The following code creates python lists to target the desired response (`col_response`) and non-composition features (`col_features`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde9a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: Obtain column names for the response and features\n",
    "col_response = [\"Fatigue Strength\"]\n",
    "col_processing = (\n",
    "    df_fatigue_data\n",
    "    >> gr.tf_drop(col_response, \"chemical_formula\")\n",
    ").columns\n",
    "\n",
    "col_processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c5c36",
   "metadata": {},
   "source": [
    "### __Q5__: Fit a regression to predict the fatigue strength\n",
    "\n",
    "Use the `ft_regression()` helper to fit a regression (with 1st order) to predict the fatigue strength.\n",
    "\n",
    "Answer the questions under *Observations* below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# TASK: Complete the code below to fit a 1st order regression\n",
    "#       use col_features as the inputs (var)\n",
    "#       use col_response as the output (out)\n",
    "###\n",
    "\n",
    "# -- FINISH THE CODE BELOW -----\n",
    "## TODO: Fit a model here\n",
    "md_fatigue = (\n",
    "\n",
    ")\n",
    "\n",
    "# -- NO NEED TO EDIT BELOW -----\n",
    "(\n",
    "    md_fatigue\n",
    "    >> gr.ev_df(df_fatigue_data)\n",
    "    >> gr.tf_summarize(\n",
    "        rmse=gr.rmse(DF[\"Fatigue Strength_mean\"], DF[\"Fatigue Strength\"]),\n",
    "        rsq=gr.rsq(DF[\"Fatigue Strength_mean\"], DF[\"Fatigue Strength\"]),\n",
    "        ndme=gr.ndme(DF[\"Fatigue Strength_mean\"], DF[\"Fatigue Strength\"]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df8898",
   "metadata": {},
   "source": [
    "*Observations*\n",
    "\n",
    "- How accurate is your model?\n",
    "  - (Your response here)\n",
    "\n",
    "- Is the model *flexible* enough to accurately capture the behavior in the data?\n",
    "  - (Your response here)\n",
    "\n",
    "- Is the way we've estimated error here *appropriate* for making decisions about the model's design? E.g. could we reasonably tune the polynomial order with this error estimate?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eaa4c7",
   "metadata": {},
   "source": [
    "### A different kind of flexibility: *Featurization*\n",
    "\n",
    "What if we wanted to use the chemical composition as a feature? We can't use the string as it is (what would $m \\times \\text{string}$ *mean*?). Instead, we can go through a process of *featurization* (also called [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering)) to create usable numeric columns that our model can use as inputs.\n",
    "\n",
    "Featurization is a *big* topic, one that is inherently problem-specific. To get us started, we're going to use a featurization scheme by [Ward et al. (2016)](https://www.nature.com/articles/npjcompumats201628?report=reader), as-implemented in the [matminer](https://hackingmaterials.lbl.gov/matminer/) python package.\n",
    "\n",
    "The following code loads a grama interface to this featurizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6209823",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the featurizer\n",
    "from grama.tran import tf_feat_composition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b907e08",
   "metadata": {},
   "source": [
    "And the following code applies the featurizer to our chemical formula column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd61d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit; featurize with the \"magpie\" featurizer\n",
    "df_fatigue_magpie = (\n",
    "    df_fatigue_data\n",
    "    >> tf_feat_composition(var_formula=\"chemical_formula\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_fatigue_magpie\n",
    "    >> gr.tf_select(gr.contains(\"Magpie\"), gr.everything())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a31a0c",
   "metadata": {},
   "source": [
    "Note that we now have over 100 new columns derived from the chemical formula. The following code picks feature columns by dropping the response column and string formula.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5a3940",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain full feature set\n",
    "col_feat_both = (\n",
    "    df_fatigue_magpie\n",
    "    >> gr.tf_drop(col_response, \"chemical_formula\")\n",
    ").columns\n",
    "\n",
    "col_feat_magpie = (\n",
    "    df_fatigue_magpie\n",
    "    >> gr.tf_select(gr.contains(\"Magpie\"))\n",
    ").columns\n",
    "\n",
    "col_feat_magpie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdcad8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f30fbb",
   "metadata": {},
   "source": [
    "### __Q6__: Interpret the following results\n",
    "\n",
    "The following code estimates model accuracy both with and without the Magpie (composition) features. Compare the results and answer the questions under *Observations* below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: No need to edit; run and answer the questions below\n",
    "df_err_processing = (\n",
    "    df_fatigue_data\n",
    "    >> gr.tf_kfolds(\n",
    "        ft=ft_regression(var=col_processing, out=col_response),\n",
    "        summaries=dict(ndme=gr.ndme),\n",
    "        seed=101,\n",
    "        k=5,\n",
    "    )\n",
    "    >> gr.tf_mutate(feats=\"processing only\")\n",
    ")\n",
    "\n",
    "df_err_magpie = (\n",
    "    df_fatigue_magpie\n",
    "    >> gr.tf_kfolds(\n",
    "        ft=ft_regression(var=col_feat_magpie, out=col_response),\n",
    "        summaries=dict(ndme=gr.ndme),\n",
    "        seed=101,\n",
    "        k=5,\n",
    "    )\n",
    "    >> gr.tf_mutate(feats=\"Magpie only\")\n",
    ")\n",
    "\n",
    "df_err_both = (\n",
    "    df_fatigue_magpie\n",
    "    >> gr.tf_kfolds(\n",
    "        ft=ft_regression(var=col_feat_both, out=col_response),\n",
    "        summaries=dict(ndme=gr.ndme),\n",
    "        seed=101,\n",
    "        k=5,\n",
    "    )\n",
    "    >> gr.tf_mutate(feats=\"processing + Magpie\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_err_processing\n",
    "    >> gr.tf_bind_rows(df_err_magpie)\n",
    "    >> gr.tf_bind_rows(df_err_both)\n",
    "    \n",
    "    >> pt.ggplot(pt.aes(\"feats\", \"ndme_Fatigue Strength\"))\n",
    "    + pt.geom_boxplot(pt.aes(group=\"feats\"))\n",
    "    + pt.scale_y_continuous(limits=[0, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be599965",
   "metadata": {},
   "source": [
    "*Observations*:\n",
    "\n",
    "- Which feature set tends to produce a more accurate model?\n",
    "  - (Your response here)\n",
    "\n",
    "- How large is the difference in accuracy between the cases?\n",
    "  - (Your response here)\n",
    "\n",
    "- How important does the chemical composition (*as expressed by Magpie features*) tend to be in this case study?\n",
    "  - (Your response here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4512fc",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "And that's it! One could certainly take this case study further (*hint*: it would be interesting to see how different the alloys are in composition...), but hopefully this already gives you a sense for the kinds of investigations you can do in materials informatics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c3fea0",
   "metadata": {},
   "source": [
    "# Survey\n",
    "\n",
    "---\n",
    "\n",
    "Once you complete this activity, please fill out the following 30-second survey:\n",
    "\n",
    "> [Survey link](https://forms.gle/XAFZeyyfiP86CCvN8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47690fa",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "---\n",
    "\n",
    "One of my favorite books on these ideas is [An Introduction to Statistical Learning](https://www.statlearning.com/). The authors have made PDF's of the first and second editions *freely available*. This book does an excellent job of speaking to non-statisticians, while also not sacrificing statistical rigor. I highly recommend it!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
